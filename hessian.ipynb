{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2860849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.nn.utils.stateless import functional_call\n",
    "\n",
    "def compute_hessian(model: nn.Module, \n",
    "                    x: torch.Tensor, \n",
    "                    y: torch.Tensor, \n",
    "                    loss_fn: callable) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Hessian of the loss w.r.t. all model parameters.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model\n",
    "        x (torch.Tensor): Input data\n",
    "        y (torch.Tensor): Target data\n",
    "        loss_fn (callable): Loss function that returns scalar (e.g., nn.MSELoss())\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Hessian matrix of shape (N, N), where N is total parameter count\n",
    "    \"\"\"\n",
    "    # Extract and flatten parameters\n",
    "    params = dict(model.named_parameters())\n",
    "    param_shapes = [(name, p.shape) for name, p in params.items()]\n",
    "    flat_params = torch.cat([p.detach().reshape(-1) for p in params.values()]).requires_grad_(True)\n",
    "\n",
    "    # Helper to unflatten flat_params into named parameter dict\n",
    "    def unflatten_params(flat_params):\n",
    "        param_dict = {}\n",
    "        idx = 0\n",
    "        for name, shape in param_shapes:\n",
    "            n = torch.tensor(shape).prod().item()\n",
    "            param_dict[name] = flat_params[idx:idx+n].view(shape)\n",
    "            idx += n\n",
    "        return param_dict\n",
    "\n",
    "    # Loss wrapper for hessian\n",
    "    def wrapped_loss(flat_params):\n",
    "        new_params = unflatten_params(flat_params)\n",
    "        y_pred = functional_call(model, new_params, (x,))\n",
    "        return loss_fn(y_pred, y)\n",
    "\n",
    "    # Compute and return Hessian\n",
    "    return hessian(wrapped_loss, flat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca52827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "tensor([[ 1.6390e+00, -9.2853e-03,  5.1009e-03,  1.6390e+00, -9.2853e-03,\n",
      "          5.1009e-03, -2.6179e+00, -4.5705e-01, -3.0191e-01,  5.8859e-01],\n",
      "        [-9.2853e-03,  1.0760e-01, -2.7343e-04, -9.2853e-03,  1.0760e-01,\n",
      "         -2.7343e-04, -1.7973e-02, -1.7112e+00,  1.6184e-02, -3.1551e-02],\n",
      "        [ 5.1009e-03, -2.7343e-04, -3.8716e-02,  5.1009e-03, -2.7343e-04,\n",
      "         -3.8716e-02,  9.8732e-03, -1.3459e-02, -3.2304e+00,  1.7333e-02],\n",
      "        [ 1.6390e+00, -9.2853e-03,  5.1009e-03,  1.6390e+00, -9.2853e-03,\n",
      "          5.1009e-03, -2.6179e+00, -4.5705e-01, -3.0191e-01,  5.8859e-01],\n",
      "        [-9.2853e-03,  1.0760e-01, -2.7343e-04, -9.2853e-03,  1.0760e-01,\n",
      "         -2.7343e-04, -1.7973e-02, -1.7112e+00,  1.6184e-02, -3.1551e-02],\n",
      "        [ 5.1009e-03, -2.7343e-04, -3.8716e-02,  5.1009e-03, -2.7343e-04,\n",
      "         -3.8716e-02,  9.8732e-03, -1.3459e-02, -3.2304e+00,  1.7333e-02],\n",
      "        [-2.6179e+00, -1.7973e-02,  9.8732e-03, -2.6179e+00, -1.7973e-02,\n",
      "          9.8732e-03,  6.4897e-01, -8.8465e-01, -5.8437e-01,  1.1393e+00],\n",
      "        [-4.5705e-01, -1.7112e+00, -1.3459e-02, -4.5705e-01, -1.7112e+00,\n",
      "         -1.3459e-02, -8.8465e-01,  1.2059e+00,  7.9659e-01, -1.5530e+00],\n",
      "        [-3.0191e-01,  1.6184e-02, -3.2304e+00, -3.0191e-01,  1.6184e-02,\n",
      "         -3.2304e+00, -5.8437e-01,  7.9659e-01,  5.2620e-01, -1.0259e+00],\n",
      "        [ 5.8859e-01, -3.1551e-02,  1.7333e-02,  5.8859e-01, -3.1551e-02,\n",
      "          1.7333e-02,  1.1393e+00, -1.5530e+00, -1.0259e+00,  2.0000e+00]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35675/2758171400.py:40: FutureWarning: `torch.nn.utils.stateless.functional_call` is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use `torch.func.functional_call` instead which is a drop-in replacement.\n",
      "  y_pred = functional_call(model, new_params, (x,))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleMLP()\n",
    "x = torch.tensor([[1.0]])\n",
    "y = torch.tensor([[2.0]])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Compute Hessian\n",
    "H = compute_hessian(model, x, y, loss_fn)\n",
    "print(H.shape)  # e.g. (10, 10)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35675/2758171400.py:40: FutureWarning: `torch.nn.utils.stateless.functional_call` is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use `torch.func.functional_call` instead which is a drop-in replacement.\n",
      "  y_pred = functional_call(model, new_params, (x,))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected:\ntensor([[9., 3.],\n        [3., 1.]])\nGot:\ntensor([[18.,  6.],\n        [ 6.,  2.]])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Hessian test passed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mtest_hessian_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtest_hessian_linear\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Compare\u001b[39;00m\n\u001b[32m     43\u001b[39m H2x2 = H[:\u001b[32m2\u001b[39m, :\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# Should be exactly 2 parameters: w and b\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(H2x2, expected, atol=\u001b[32m1e-6\u001b[39m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGot:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mH2x2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Hessian test passed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Expected:\ntensor([[9., 3.],\n        [3., 1.]])\nGot:\ntensor([[18.,  6.],\n        [ 6.,  2.]])"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Assume compute_hessian is already defined\n",
    "\n",
    "# Define simple linear model: y = wx + b\n",
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def test_hessian_linear():\n",
    "    # Input and target\n",
    "    x_val = 3.0\n",
    "    x = torch.tensor([[x_val]])\n",
    "    y = torch.tensor([[1.0]])\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    def loss_fn(y_pred, y):\n",
    "        return 0.5 * ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "    # Model\n",
    "    model = SimpleLinear()\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.fill_(0.0)\n",
    "        model.linear.bias.fill_(0.0)\n",
    "\n",
    "    # Compute Hessian\n",
    "    H = compute_hessian(model, x, y, loss_fn)\n",
    "\n",
    "    # Expected Hessian:\n",
    "    # d^2L/dw^2 = x^2\n",
    "    # d^2L/dwdb = x\n",
    "    # d^2L/db^2 = 1\n",
    "    x_sq = x_val ** 2\n",
    "    expected = torch.tensor([\n",
    "        [x_sq, x_val],\n",
    "        [x_val, 1.0]\n",
    "    ])\n",
    "\n",
    "    # Compare\n",
    "    H2x2 = H[:2, :2]  # Should be exactly 2 parameters: w and b\n",
    "    assert torch.allclose(H2x2, expected, atol=1e-6), f\"Expected:\\n{expected}\\nGot:\\n{H2x2}\"\n",
    "    print(\"✅ Hessian test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_hessian_linear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8ddbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
